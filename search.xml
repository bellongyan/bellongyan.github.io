<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>PaddlePaddle 飞桨实现GAN生成对抗网络生成MINIST手写数字图像</title>
      <link href="/p/2023/GanGeneratePics/"/>
      <url>/p/2023/GanGeneratePics/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> paddle.nn <span class="keyword">import</span> Conv2D, MaxPool2D, Linear, BatchNorm,Upsample</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><pre><code>/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:26: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations  def convert_to_list(value, n, name, dtype=np.int):/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/__init__.py:107: DeprecationWarning: Using or importing the ABCs from &#39;collections&#39; instead of from &#39;collections.abc&#39; is deprecated, and in 3.8 it will stop working  from collections import MutableMapping/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/rcsetup.py:20: DeprecationWarning: Using or importing the ABCs from &#39;collections&#39; instead of from &#39;collections.abc&#39; is deprecated, and in 3.8 it will stop working  from collections import Iterable, Mapping/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/colors.py:53: DeprecationWarning: Using or importing the ABCs from &#39;collections&#39; instead of from &#39;collections.abc&#39; is deprecated, and in 3.8 it will stop working  from collections import Sized</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_reader = paddle.vision.datasets.MNIST(mode=<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练集样本数:&quot;</span>, <span class="built_in">len</span>(train_reader))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;样本形状:&quot;</span>, np.array(train_reader[<span class="number">0</span>][<span class="number">0</span>]).shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;标签形状:&quot;</span>, np.array(train_reader[<span class="number">0</span>][<span class="number">1</span>]).shape)</span><br></pre></td></tr></table></figure><pre><code>训练集样本数: 60000样本形状: (28, 28)标签形状: (1,)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 噪声维度</span></span><br><span class="line">Z_DIM = <span class="number">100</span></span><br><span class="line">BATCH_SIZE = <span class="number">128</span></span><br><span class="line"></span><br><span class="line">paddle.vision.set_image_backend(<span class="string">&#x27;cv2&#x27;</span>)</span><br><span class="line">mnist_generator = paddle.io.DataLoader(paddle.vision.datasets.MNIST(mode=<span class="string">&#x27;train&#x27;</span>),</span><br><span class="line">                                       batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成假图片的reader, 噪声生成，通过由噪声来生成假的图片数据输入</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ZReader</span>(paddle.io.Dataset):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, z_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(ZReader, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.z_dim = z_dim</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="keyword">return</span> np.random.normal(<span class="number">0.0</span>, <span class="number">1.0</span>, (<span class="variable language_">self</span>.z_dim,<span class="number">1</span>,<span class="number">1</span>)).astype(<span class="string">&#x27;float32&#x27;</span>)<span class="comment">#正态分布，正态分布的均值、标准差、参数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">int</span>(<span class="number">1e8</span>)  <span class="comment"># a large number</span></span><br><span class="line"></span><br><span class="line">z_dataset = ZReader(Z_DIM)</span><br><span class="line">z_generator = paddle.io.DataLoader(z_dataset, batch_size=BATCH_SIZE)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">image,label =<span class="built_in">next</span>(mnist_generator())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;图像数据形状和对应数据为:&quot;</span>, image.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;图像标签形状和对应数据为:&quot;</span>, label.shape)</span><br><span class="line"></span><br><span class="line">plt.imshow(image[<span class="number">0</span>].numpy().astype(np.uint8)) <span class="comment"># (28,28)</span></span><br><span class="line">plt.show</span><br></pre></td></tr></table></figure><pre><code>/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py:89: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations  if isinstance(slot[0], (np.ndarray, np.bool, numbers.Number)):图像数据形状和对应数据为: [128, 28, 28]图像标签形状和对应数据为: [128, 1]W0511 14:09:22.929143   892 device_context.cc:362] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 10.1W0511 14:09:22.933797   892 device_context.cc:372] device: 0, cuDNN Version: 7.6./opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/cbook/__init__.py:2349: DeprecationWarning: Using or importing the ABCs from &#39;collections&#39; instead of from &#39;collections.abc&#39; is deprecated, and in 3.8 it will stop working  if isinstance(obj, collections.Iterator):/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/cbook/__init__.py:2366: DeprecationWarning: Using or importing the ABCs from &#39;collections&#39; instead of from &#39;collections.abc&#39; is deprecated, and in 3.8 it will stop working  return list(data) if isinstance(data, collections.MappingView) else data&lt;function matplotlib.pyplot.show(*args, **kw)&gt;/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/image.py:425: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead  a_min = np.asscalar(a_min.astype(scaled_dtype))/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/image.py:426: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead  a_max = np.asscalar(a_max.astype(scaled_dtype))</code></pre><p><img src="2770491-20230511144847090-516948239.png" alt="img"></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">##测试一下噪声</span></span><br><span class="line">z_tmp = <span class="built_in">next</span>(z_generator())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;一个batch噪声z的形状：&#x27;</span>, z_tmp[<span class="number">0</span>].shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>一个batch噪声z的形状： [128, 100, 1, 1]</code></pre><h2 id="GAN-网络"><a href="#GAN-网络" class="headerlink" title="GAN 网络"></a>GAN 网络</h2><p>GAN 性能的提升从生成器 G 和判别器 D 进行左右互搏、交替完善的过程得到的。所以其 G 网络和 D 网络的能力应该设计得相近，复杂度也差不多。这个项目中的生成器，采用了两个全链接层接两组上采样和转置卷积层，将输入的噪声 Z 逐渐转化为 1×28×28 的单通道图片输出。</p><p>生成器结构：</p><p><img src="https://ai-studio-static-online.cdn.bcebos.com/625f6642177144c49bcd01b05a78e19094fe74d0f51a4cdb989648d3adcbe362" style="height:400px;width:600px"></p><p>判别器的结构正好相反，先通过两组卷积和池化层将输入的图片转化为越来越小的特征图，再经过两层全链接层，输出图片是真是假的二分类结果。</p><p>判别器结构：<br><br/></p><p><img src="https://ai-studio-static-online.cdn.bcebos.com/d3a0dc774d0d41758bc674deb189a76e653815de352c4e6d951807be13baec91" style="height:400px;width:600px"></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 判别器 D</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">D</span>(paddle.nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, name_scope</span>):</span><br><span class="line">        <span class="built_in">super</span>(D, <span class="variable language_">self</span>).__init__(name_scope)</span><br><span class="line">        name_scope = <span class="variable language_">self</span>.full_name()</span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># My_D的代码</span></span><br><span class="line">        <span class="variable language_">self</span>.conv1 = paddle.fluid.Conv2D(num_channels=<span class="number">1</span>, num_filters=<span class="number">64</span>, filter_size=<span class="number">3</span>)</span><br><span class="line">        <span class="variable language_">self</span>.bn1 = paddle.nn.BatchNorm(num_channels=<span class="number">64</span>, act=<span class="string">&#x27;leaky_relu&#x27;</span>)</span><br><span class="line">        <span class="variable language_">self</span>.pool1 = paddle.fluid.Pool2D(pool_size=<span class="number">2</span>, pool_stride=<span class="number">2</span>)</span><br><span class="line">        <span class="variable language_">self</span>.conv2 = paddle.fluid.Conv2D(num_channels=<span class="number">64</span>, num_filters=<span class="number">128</span>, filter_size=<span class="number">3</span>)</span><br><span class="line">        <span class="variable language_">self</span>.bn2 = paddle.nn.BatchNorm(num_channels=<span class="number">128</span>, act=<span class="string">&#x27;leaky_relu&#x27;</span>)</span><br><span class="line">        <span class="variable language_">self</span>.pool2 = paddle.fluid.Pool2D(pool_size=<span class="number">2</span>, pool_stride=<span class="number">2</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc1 = paddle.fluid.Linear(input_dim=<span class="number">128</span> * <span class="number">5</span> * <span class="number">5</span>, output_dim=<span class="number">1024</span>)</span><br><span class="line">        <span class="variable language_">self</span>.bnfc1 = paddle.nn.BatchNorm(num_channels=<span class="number">1024</span>, act=<span class="string">&#x27;leaky_relu&#x27;</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc2 = paddle.fluid.Linear(input_dim=<span class="number">1024</span>, output_dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, img</span>):</span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># My_G forward的代码</span></span><br><span class="line">        y = <span class="variable language_">self</span>.conv1(img)</span><br><span class="line">        y = <span class="variable language_">self</span>.bn1(y)</span><br><span class="line">        y = <span class="variable language_">self</span>.pool1(y)</span><br><span class="line">        y = <span class="variable language_">self</span>.conv2(y)</span><br><span class="line">        y = <span class="variable language_">self</span>.bn2(y)</span><br><span class="line">        y = <span class="variable language_">self</span>.pool2(y)</span><br><span class="line">        y = paddle.reshape(x=y, shape=[-<span class="number">1</span>, <span class="number">128</span> * <span class="number">5</span> * <span class="number">5</span>])</span><br><span class="line">        y = <span class="variable language_">self</span>.fc1(y)</span><br><span class="line">        y = <span class="variable language_">self</span>.bnfc1(y)</span><br><span class="line">        y = <span class="variable language_">self</span>.fc2(y)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成网络G</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">G</span>(paddle.nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, name_scope</span>):</span><br><span class="line">        <span class="built_in">super</span>(G, <span class="variable language_">self</span>).__init__(name_scope)</span><br><span class="line">        name_scope = <span class="variable language_">self</span>.full_name()</span><br><span class="line">        <span class="comment"># 第一组全连接和BN层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = paddle.fluid.Linear(input_dim=<span class="number">100</span>, output_dim=<span class="number">1024</span>)</span><br><span class="line">        <span class="variable language_">self</span>.bn1 = paddle.nn.BatchNorm(num_channels=<span class="number">1024</span>, act=<span class="string">&#x27;tanh&#x27;</span>)</span><br><span class="line">        <span class="comment"># 第二组全连接和BN层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = paddle.fluid.Linear(input_dim=<span class="number">1024</span>, output_dim=<span class="number">128</span> * <span class="number">7</span> * <span class="number">7</span>)</span><br><span class="line">        <span class="variable language_">self</span>.bn2 = paddle.nn.BatchNorm(num_channels=<span class="number">128</span> * <span class="number">7</span> * <span class="number">7</span>, act=<span class="string">&#x27;tanh&#x27;</span>)</span><br><span class="line">        <span class="comment"># 第一组卷积运算（卷积前进行上采样，以扩大特征图）</span></span><br><span class="line">        <span class="comment"># 注：此处使用转置卷积的效果似乎不如上采样后直接用卷积，转置卷积生成的图片噪点较多</span></span><br><span class="line">        <span class="variable language_">self</span>.conv1 = paddle.fluid.Conv2D(num_channels=<span class="number">128</span>, num_filters=<span class="number">64</span>, filter_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        <span class="variable language_">self</span>.bn3 = paddle.nn.BatchNorm(num_channels=<span class="number">64</span>, act=<span class="string">&#x27;tanh&#x27;</span>)</span><br><span class="line">        <span class="comment"># 第二组卷积运算（卷积前进行上采样，以扩大特征图）</span></span><br><span class="line">        <span class="variable language_">self</span>.conv2 = paddle.fluid.Conv2D(num_channels=<span class="number">64</span>, num_filters=<span class="number">1</span>, filter_size=<span class="number">5</span>, padding=<span class="number">2</span>, act=<span class="string">&#x27;tanh&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, z</span>):</span><br><span class="line">        z = paddle.reshape(x=z, shape=[-<span class="number">1</span>, <span class="number">100</span>])</span><br><span class="line">        y = <span class="variable language_">self</span>.fc1(z)</span><br><span class="line">        y = <span class="variable language_">self</span>.bn1(y)</span><br><span class="line">        y = <span class="variable language_">self</span>.fc2(y)</span><br><span class="line">        y = <span class="variable language_">self</span>.bn2(y)</span><br><span class="line">        y = paddle.reshape(x=y, shape=[-<span class="number">1</span>, <span class="number">128</span>, <span class="number">7</span>, <span class="number">7</span>])</span><br><span class="line">        <span class="comment"># 第一组卷积前进行上采样以扩大特征图</span></span><br><span class="line">        y = paddle.fluid.layers.image_resize(y, scale=<span class="number">2</span>)</span><br><span class="line">        y = <span class="variable language_">self</span>.conv1(y)</span><br><span class="line">        y = <span class="variable language_">self</span>.bn3(y)</span><br><span class="line">        <span class="comment"># 第二组卷积前进行上采样以扩大特征图</span></span><br><span class="line">        y = paddle.fluid.layers.image_resize(y, scale=<span class="number">2</span>)</span><br><span class="line">        y = <span class="variable language_">self</span>.conv2(y)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">paddle.Model(G(<span class="string">&#x27;G&#x27;</span>)).summary((-<span class="number">1</span>,<span class="number">100</span>))</span><br></pre></td></tr></table></figure><pre><code>--------------------------------------------------------------------------- Layer (type)       Input Shape          Output Shape         Param #===========================================================================   Linear-1          [[1, 100]]           [1, 1024]           103,424  BatchNorm-1       [[1, 1024]]           [1, 1024]            4,096   Linear-2         [[1, 1024]]           [1, 6272]          6,428,800  BatchNorm-2       [[1, 6272]]           [1, 6272]           25,088   Conv2D-1      [[1, 128, 14, 14]]    [1, 64, 14, 14]        204,864  BatchNorm-3    [[1, 64, 14, 14]]     [1, 64, 14, 14]          256   Conv2D-2      [[1, 64, 28, 28]]      [1, 1, 28, 28]         1,601===========================================================================Total params: 6,768,129Trainable params: 6,738,689Non-trainable params: 29,440---------------------------------------------------------------------------Input size (MB): 0.00Forward/backward pass size (MB): 0.31Params size (MB): 25.82Estimated Total Size (MB): 26.13---------------------------------------------------------------------------&#123;&#39;total_params&#39;: 6768129, &#39;trainable_params&#39;: 6738689&#125;</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">paddle.Model(D(<span class="string">&#x27;D&#x27;</span>)).summary((-<span class="number">1</span>,<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>))</span><br></pre></td></tr></table></figure><pre><code>--------------------------------------------------------------------------- Layer (type)       Input Shape          Output Shape         Param #===========================================================================   Conv2D-3       [[1, 1, 28, 28]]     [1, 64, 26, 26]          640  BatchNorm-4    [[1, 64, 26, 26]]     [1, 64, 26, 26]          256   Pool2D-1      [[1, 64, 26, 26]]     [1, 64, 13, 13]           0   Conv2D-4      [[1, 64, 13, 13]]     [1, 128, 11, 11]       73,856  BatchNorm-5    [[1, 128, 11, 11]]    [1, 128, 11, 11]         512   Pool2D-2      [[1, 128, 11, 11]]     [1, 128, 5, 5]           0   Linear-3         [[1, 3200]]           [1, 1024]          3,277,824  BatchNorm-6       [[1, 1024]]           [1, 1024]            4,096   Linear-4         [[1, 1024]]             [1, 1]             1,025===========================================================================Total params: 3,358,209Trainable params: 3,353,345Non-trainable params: 4,864---------------------------------------------------------------------------Input size (MB): 0.00Forward/backward pass size (MB): 1.02Params size (MB): 12.81Estimated Total Size (MB): 13.83---------------------------------------------------------------------------&#123;&#39;total_params&#39;: 3358209, &#39;trainable_params&#39;: 3353345&#125;</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">z_tmp = <span class="built_in">next</span>(z_generator())</span><br><span class="line">g_tmp = G(<span class="string">&#x27;G&#x27;</span>)</span><br><span class="line">tmp_g = g_tmp(z_tmp[<span class="number">0</span>]).numpy()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;生成器G生成图片数据的形状：&#x27;</span>, tmp_g.shape)</span><br><span class="line">plt.imshow(tmp_g[<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">d_tmp = D(<span class="string">&#x27;D&#x27;</span>)</span><br><span class="line">tmp_d = d_tmp(paddle.to_tensor(tmp_g)).numpy()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;判别器D判别生成的图片的概率数据形状：&#x27;</span>, tmp_d.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">max</span>(tmp_d))</span><br></pre></td></tr></table></figure><pre><code>生成器G生成图片数据的形状： (128, 1, 28, 28)判别器D判别生成的图片的概率数据形状： (128, 1)[3.0552034]</code></pre><p><img src="2770491-20230511144914487-982094820.png" alt="img"></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_image_grid</span>(<span class="params">images, batch_size=<span class="number">128</span>, pass_id=<span class="literal">None</span></span>):</span><br><span class="line">    fig = plt.figure(figsize=(<span class="number">8</span>, batch_size/<span class="number">32</span>))</span><br><span class="line">    gs = plt.GridSpec(<span class="built_in">int</span>(batch_size/<span class="number">16</span>), <span class="number">16</span>)</span><br><span class="line">    gs.update(wspace=<span class="number">0.05</span>, hspace=<span class="number">0.05</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, image <span class="keyword">in</span> <span class="built_in">enumerate</span>(images):</span><br><span class="line">        ax = plt.subplot(gs[i])</span><br><span class="line">        plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">        ax.set_xticklabels([])</span><br><span class="line">        ax.set_yticklabels([])</span><br><span class="line">        ax.set_aspect(<span class="string">&#x27;equal&#x27;</span>)</span><br><span class="line">        plt.imshow(image[<span class="number">0</span>], cmap=<span class="string">&#x27;Greys_r&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">show_image_grid(tmp_g, BATCH_SIZE)</span><br></pre></td></tr></table></figure><p><img src="2770491-20230511144928191-1649608948.png" alt="img"></p><h2 id="网络训练"><a href="#网络训练" class="headerlink" title="网络训练"></a>网络训练</h2><p>网络的训练优化目标就是如下公式：</p><script type="math/tex; mode=display">\min _{G} \max _{D} V(D, G)=\mathbb{E}_{\boldsymbol{x} \sim p_{\text {data } }(\boldsymbol{x})}[\log D(\boldsymbol{x})]+\mathbb{E}_{\boldsymbol{z} \sim p_{\boldsymbol{z} }(\boldsymbol{z})}[\log (1-D(G(\boldsymbol{z})))]</script><p>公式出自 Goodfellow 在 2014 年发表的论文<a href="https://arxiv.org/pdf/1406.2661v1.pdf">Generative Adversarial Nets</a>。<br>这里简单介绍下公式的含义和如何应用到代码中。上式中等号左边的部分：</p><p>$V(D, G)$表示的是生成样本和真实样本的差异度，可以使用二分类（真、假两个类别）的交叉商损失。</p><p>$\max _{D} V(D,G)$表示在生成器固定的情况下，通过最大化交叉商损失$V(D, G)$来更新判别器 D 的参数。</p><p>$\min _{G} \max _{D} V(D, G)$表示生成器要在判别器最大化真、假图片交叉商损失$\max _{D} V(D,G)$的情况下，最小化这个交叉商损失。</p><p>等式的右边其实就是将等式左边的交叉商损失公式展开，并写成概率分布的期望形式。详细的推导请参见原论文《<a href="https://arxiv.org/pdf/1406.2661v1.pdf">Generative Adversarial Nets</a>》。</p><p>下面是训练模型的代码，有详细的注释。大致过程是：先用真图片训练一次判别器 d 的参数，再用生成器 g 生成的假图片训练一次判别器 d 的参数，最后用判别器 d 判断生成器 g 生成的假图片的概率值更新一次生成器 g 的参数，即每轮训练先训练两次判别器 d，再训练一次生成器 g，使得判别器 d 的能力始终稍稍高于生成器 g 一些。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i, real_image <span class="keyword">in</span> <span class="built_in">enumerate</span>(mnist_generator()):</span><br><span class="line">    <span class="built_in">print</span>(real_image[<span class="number">0</span>].shape, <span class="built_in">len</span>(real_image[<span class="number">0</span>]))</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure><pre><code>[128, 28, 28] 128</code></pre><h2 id="训练代码"><a href="#训练代码" class="headerlink" title="训练代码"></a>训练代码</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle.nn.functional <span class="keyword">as</span> F</span><br><span class="line">d = D(<span class="string">&#x27;D&#x27;</span>)</span><br><span class="line">d.train()</span><br><span class="line">g = G(<span class="string">&#x27;G&#x27;</span>)</span><br><span class="line">g.train()</span><br><span class="line"><span class="comment"># 创建优化方法</span></span><br><span class="line">real_d_optimizer = paddle.optimizer.Adam(learning_rate=<span class="number">1e-4</span>, parameters=d.parameters())</span><br><span class="line">fake_d_optimizer = paddle.optimizer.Adam(learning_rate=<span class="number">1e-4</span>, parameters=d.parameters())</span><br><span class="line">g_optimizer = paddle.optimizer.Adam(learning_rate=<span class="number">5e-4</span>, parameters=g.parameters())</span><br><span class="line"></span><br><span class="line">iteration_num = <span class="number">0</span></span><br><span class="line">epoch_num = <span class="number">5</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epoch_num):</span><br><span class="line">    <span class="keyword">for</span> i, (real_image, label) <span class="keyword">in</span> <span class="built_in">enumerate</span>(mnist_generator()):</span><br><span class="line">        <span class="comment"># 丢弃不满整个batch_size的数据</span></span><br><span class="line">        <span class="keyword">if</span>(<span class="built_in">len</span>(real_image) != BATCH_SIZE):</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        iteration_num += <span class="number">1</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        判别器d通过最小化输入真实图片时判别器d的输出与真值标签ones的交叉熵损失，来优化判别器的参数，</span></span><br><span class="line"><span class="string">        以增加判别器d识别真实图片real_image为真值标签ones的概率。</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># 将MNIST数据集里的图片读入real_image，将真值标签ones用数字1初始化</span></span><br><span class="line">        real_image = real_image.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        real_image = paddle.to_tensor(real_image)</span><br><span class="line">        ones = paddle.to_tensor(np.ones([<span class="built_in">len</span>(real_image), <span class="number">1</span>]).astype(<span class="string">&#x27;float32&#x27;</span>))</span><br><span class="line">        <span class="comment"># 计算判别器d判断真实图片的概率</span></span><br><span class="line">        p_real = d(real_image)</span><br><span class="line">        <span class="comment"># 计算判别真图片为真的损失</span></span><br><span class="line">        real_cost = F.binary_cross_entropy_with_logits(p_real, ones)</span><br><span class="line">        real_avg_cost = paddle.mean(real_cost)</span><br><span class="line">        <span class="comment"># 反向传播更新判别器d的参数</span></span><br><span class="line">        real_avg_cost.backward()</span><br><span class="line">        real_d_optimizer.minimize(real_avg_cost)</span><br><span class="line">        d.clear_gradients()</span><br><span class="line"></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        判别器d通过最小化输入生成器g生成的假图片g(z)时判别器的输出与假值标签zeros的交叉熵损失，</span></span><br><span class="line"><span class="string">        来优化判别器d的参数，以增加判别器d识别生成器g生成的假图片g(z)为假值标签zeros的概率。</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># 创建高斯分布的噪声z，将假值标签zeros初始化为0</span></span><br><span class="line">        z = <span class="built_in">next</span>(z_generator())</span><br><span class="line">        zeros = paddle.to_tensor(np.zeros([<span class="built_in">len</span>(real_image), <span class="number">1</span>]).astype(<span class="string">&#x27;float32&#x27;</span>))</span><br><span class="line">        <span class="comment"># 判别器d判断生成器g生成的假图片的概率</span></span><br><span class="line">        p_fake = d(g(z[<span class="number">0</span>]))</span><br><span class="line">        <span class="comment"># 计算判别生成器g生成的假图片为假的损失</span></span><br><span class="line">        fake_cost = F.binary_cross_entropy_with_logits(p_fake, zeros)</span><br><span class="line">        fake_avg_cost = paddle.mean(fake_cost)</span><br><span class="line">        <span class="comment"># 反向传播更新判别器d的参数</span></span><br><span class="line">        fake_avg_cost.backward()</span><br><span class="line">        fake_d_optimizer.minimize(fake_avg_cost)</span><br><span class="line">        d.clear_gradients()</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        生成器g通过最小化判别器d判别生成器生成的假图片g(z)为真的概率d(fake)与真值标签ones的交叉熵损失，</span></span><br><span class="line"><span class="string">        来优化生成器g的参数，以增加生成器g使判别器d判别其生成的假图片g(z)为真值标签ones的概率。</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># 生成器用输入的高斯噪声z生成假图片</span></span><br><span class="line">        fake = g(z[<span class="number">0</span>])</span><br><span class="line">        <span class="comment"># 计算判别器d判断生成器g生成的假图片的概率</span></span><br><span class="line">        p_confused = d(fake)</span><br><span class="line">        <span class="comment"># 使用判别器d判断生成器g生成的假图片的概率与真值ones的交叉熵计算损失</span></span><br><span class="line">        g_cost = paddle.fluid.layers.sigmoid_cross_entropy_with_logits(p_confused, ones)</span><br><span class="line">        g_avg_cost = paddle.mean(x=g_cost)</span><br><span class="line">        <span class="comment"># 反向传播更新生成器g的参数</span></span><br><span class="line">        g_avg_cost.backward()</span><br><span class="line">        g_optimizer.minimize(g_avg_cost)</span><br><span class="line">        g.clear_gradients()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 打印输出</span></span><br><span class="line">        <span class="keyword">if</span>(iteration_num % <span class="number">100</span> == <span class="number">0</span>):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;epoch =&#x27;</span>, epoch, <span class="string">&#x27;, batch =&#x27;</span>, i, <span class="string">&#x27;, real_d_loss =&#x27;</span>, real_avg_cost.numpy(),</span><br><span class="line">                <span class="string">&#x27;, fake_d_loss =&#x27;</span>, fake_avg_cost.numpy(), <span class="string">&#x27;g_loss =&#x27;</span>, g_avg_cost.numpy())</span><br><span class="line">            show_image_grid(fake.numpy(), BATCH_SIZE, epoch)</span><br><span class="line">            <span class="comment"># 存储模型</span></span><br><span class="line">paddle.save(g.state_dict(), <span class="string">&#x27;./output/&#x27;</span>+<span class="string">&#x27;g&#x27;</span>)</span><br><span class="line">paddle.save(d.state_dict(), <span class="string">&#x27;./output/&#x27;</span>+<span class="string">&#x27;d_o_r&#x27;</span>)</span><br><span class="line">paddle.save(d.state_dict(), <span class="string">&#x27;./output/&#x27;</span>+<span class="string">&#x27;d_o_f&#x27;</span>)</span><br></pre></td></tr></table></figure><pre><code>epoch = 0 , batch = 99 , real_d_loss = [0.63881606] , fake_d_loss = [0.43077955] g_loss = [1.1314435]/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/image.py:425: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead  a_min = np.asscalar(a_min.astype(scaled_dtype))/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/image.py:426: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead  a_max = np.asscalar(a_max.astype(scaled_dtype))</code></pre><p><img src="2770491-20230511144950277-1324896489.png" alt="img"></p><pre><code>epoch = 0 , batch = 199 , real_d_loss = [0.4727745] , fake_d_loss = [0.40940467] g_loss = [1.1723493]</code></pre><p><img src="2770491-20230511145002195-1203362519.png" alt="img"></p><pre><code>epoch = 0 , batch = 299 , real_d_loss = [0.3923043] , fake_d_loss = [0.38755333] g_loss = [1.2083172]</code></pre><p><img src="2770491-20230511145011033-1880770436.png" alt="img"></p><pre><code>epoch = 0 , batch = 399 , real_d_loss = [0.3553782] , fake_d_loss = [0.3865837] g_loss = [1.2023909]</code></pre><p><img src="2770491-20230511145112574-582443905.png" alt="img"></p><pre><code>epoch = 1 , batch = 31 , real_d_loss = [0.30362654] , fake_d_loss = [0.3808377] g_loss = [1.22538]</code></pre><p><img src="2770491-20230511145447740-1001371440.png" alt="img"></p><pre><code>epoch = 1 , batch = 131 , real_d_loss = [0.3126711] , fake_d_loss = [0.38687515] g_loss = [1.2130171]</code></pre><p><img src="2770491-20230511145458301-1143052080.png" alt="img"></p><pre><code>epoch = 1 , batch = 231 , real_d_loss = [0.273176] , fake_d_loss = [0.37318167] g_loss = [1.2494049]</code></pre><p><img src="2770491-20230511145508808-239873142.png" alt="img"></p><pre><code>epoch = 1 , batch = 331 , real_d_loss = [0.40979803] , fake_d_loss = [0.4624972] g_loss = [1.1285127]</code></pre><p><img src="2770491-20230511145520269-1543039206.png" alt="img"></p><pre><code>epoch = 1 , batch = 431 , real_d_loss = [0.5482184] , fake_d_loss = [0.4858724] g_loss = [1.1077986]</code></pre><p><img src="2770491-20230511145531437-1634388380.png" alt="img"></p><pre><code>epoch = 2 , batch = 63 , real_d_loss = [0.5916477] , fake_d_loss = [0.5695666] g_loss = [0.9861771]</code></pre><p><img src="2770491-20230511145541092-1832221091.png" alt="img"></p><pre><code>epoch = 2 , batch = 163 , real_d_loss = [0.6595901] , fake_d_loss = [0.54358363] g_loss = [1.0098863]</code></pre><p><img src="2770491-20230511145550635-670965453.png" alt="img"></p><pre><code>epoch = 2 , batch = 263 , real_d_loss = [0.7349342] , fake_d_loss = [0.5123524] g_loss = [1.0610704]</code></pre><p><img src="2770491-20230511145600776-438275988.png" alt="img"></p><pre><code>epoch = 2 , batch = 363 , real_d_loss = [0.88330436] , fake_d_loss = [0.51227736] g_loss = [1.0595765]</code></pre><p><img src="2770491-20230511145619362-2032834226.png" alt="img"></p><pre><code>epoch = 2 , batch = 463 , real_d_loss = [0.83836365] , fake_d_loss = [0.53332305] g_loss = [0.99540174]</code></pre><p><img src="2770491-20230511145644891-56212330.png" alt="img"></p><pre><code>epoch = 3 , batch = 95 , real_d_loss = [0.86019707] , fake_d_loss = [0.53724575] g_loss = [0.9913846]</code></pre><p><img src="2770491-20230511145656006-1357629608.png" alt="img"></p><pre><code>epoch = 3 , batch = 195 , real_d_loss = [0.8524647] , fake_d_loss = [0.5335064] g_loss = [1.0278957]</code></pre><p><img src="2770491-20230511145706060-828861569.png" alt="img"></p><pre><code>epoch = 3 , batch = 295 , real_d_loss = [0.90599334] , fake_d_loss = [0.5457525] g_loss = [0.9990233]</code></pre><p><img src="2770491-20230511145719315-1020039163.png" alt="img"></p><pre><code>epoch = 3 , batch = 395 , real_d_loss = [0.7981782] , fake_d_loss = [0.539215] g_loss = [0.98800594]</code></pre><p><img src="2770491-20230511145730244-593632313.png" alt="img"></p><pre><code>epoch = 4 , batch = 27 , real_d_loss = [0.8963315] , fake_d_loss = [0.5651827] g_loss = [0.94507515]</code></pre><p><img src="2770491-20230511145742890-1745870663.png" alt="img"></p><pre><code>epoch = 4 , batch = 127 , real_d_loss = [0.8121051] , fake_d_loss = [0.61096513] g_loss = [0.8886054]</code></pre><p><img src="2770491-20230511145752307-1233940078.png" alt="img"></p><pre><code>epoch = 4 , batch = 227 , real_d_loss = [0.7931969] , fake_d_loss = [0.5683154] g_loss = [0.9184775]</code></pre><p><img src="2770491-20230511145804071-550907269.png" alt="img"></p><pre><code>epoch = 4 , batch = 327 , real_d_loss = [0.8052078] , fake_d_loss = [0.55665344] g_loss = [0.947585]</code></pre><p><img src="2770491-20230511145813128-1035986812.png" alt="img"></p><pre><code>epoch = 4 , batch = 427 , real_d_loss = [0.9077438] , fake_d_loss = [0.52213967] g_loss = [0.9807905]</code></pre><p><img src="2770491-20230511145822294-1719077370.png" alt="img"></p><h2 id="生成测试"><a href="#生成测试" class="headerlink" title="生成测试"></a>生成测试</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">z = <span class="built_in">next</span>(z_generator())</span><br><span class="line">fake = g(z[<span class="number">0</span>])</span><br><span class="line">show_image_grid(fake.numpy(), <span class="number">128</span>, <span class="number">0</span>)</span><br></pre></td></tr></table></figure><p><img src="2770491-20230511145831836-214151350.png" alt="img"></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> paddlepaddle </tag>
            
            <tag> 飞桨 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>别玩手机 图像分类比赛</title>
      <link href="/p/2023/DoNotPlayPhone/"/>
      <url>/p/2023/DoNotPlayPhone/</url>
      
        <content type="html"><![CDATA[<p><a href="https://aistudio.baidu.com/aistudio/competition/detail/934/0/introduction">浙大宁波理工学院数据科学与大数据专业 别玩手机图像分类比赛</a></p><p>1 选手禁止互相抄袭，发现结果雷同者将取消成绩；</p><p>2 请在基线模型基础上修改代码，不允许使用第三方封装库、套件或者其他工具，否则做 0 分处理；</p><p>3 每位同学请独立完成比赛，不允许就比赛技术问题进行相互交流，更不允许索要代码，请自觉遵守规则，保持良好的品格；</p><p>4 晚上 12：00 以后不允许递交，否则做 0 分处理；</p><p>5 结果文件必须是程序生成，不允许手动修改或者后期处理。</p><hr><h2 id="赛题背景"><a href="#赛题背景" class="headerlink" title="赛题背景"></a>赛题背景</h2><p>如今，手机已成为大众离不开的生活工具，而且它的迅速发展使得它的功能不再以通讯为主，手机逐渐发展为可移动的大众传播媒体终端设备，甚至可以比作为第五媒体。当今的大学生群体是智能手机使用者中的一支巨大的的队伍，零零后大学生在进入大学以来，学习生活中过度的依赖手机，甚至上课时忘记携带手机便会手足无措，神情恍惚。本比赛要求通过监控摄像头等拍摄到的画面判断画面中的人物是否正在使用手机</p><hr><h2 id="数据集介绍"><a href="#数据集介绍" class="headerlink" title="数据集介绍"></a>数据集介绍</h2><p>本比赛采用的数据集中,训练集共 2180 张使用手机的图片（位于目录 data/data146247/train/0_phone/）、1971 张没有使用手机的图片（位于目录 data/data146247/train/1_no_phone/）<br>测试集共 1849 张图片，无标注信息</p><hr><h2 id="总体思路"><a href="#总体思路" class="headerlink" title="总体思路"></a>总体思路</h2><p>本 Baseline 采用 LeNet 模型架构，参赛者可以在此基础上进行修改，也可以使用全新的网络架构冲榜~~<br>LeNet 网络结构图如下<br><img src="http://cuijiahua.com/wp-content/uploads/2018/01/dl_3_4.jpg" alt="image.png"><br>LeNet-5 共有 7 层，不包含输入，每层都包含可训练参数；每个层有多个 Feature Map，每个 FeatureMap 通过一种卷积滤波器提取输入的一种特征，然后每个 FeatureMap 有多个神经元。</p><ol><li>预处理<ul><li>生成标签</li><li>读取数据集</li></ul></li><li>训练<ul><li>模型组网</li><li>反向传播</li></ul></li><li>预测<ul><li>模型预测并保存结果</li></ul></li></ol><p><code>in[1]</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">!rm aug.csv</span><br><span class="line">!rm test.csv</span><br><span class="line">!rm train.csv</span><br><span class="line">!rm -r augment</span><br><span class="line">!rm -r train</span><br><span class="line">!rm -r test</span><br></pre></td></tr></table></figure><p><code>in[2]</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">!unzip -oq /home/aistudio/data/data146247/train.<span class="built_in">zip</span></span><br><span class="line">!unzip -oq /home/aistudio/data/data146247/test.<span class="built_in">zip</span>  <span class="comment">#解压数据集</span></span><br><span class="line">!mkdir augment</span><br><span class="line">!mkdir augment/0_phone</span><br><span class="line">!mkdir augment/1_no_phone</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><code>in[3]</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> paddle.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> paddle.vision.transforms <span class="keyword">as</span> T</span><br><span class="line"><span class="keyword">from</span> paddle.nn <span class="keyword">import</span> Conv2D, BatchNorm2D, Linear, Dropout</span><br><span class="line"><span class="keyword">from</span> paddle.nn <span class="keyword">import</span> AdaptiveAvgPool2D, MaxPool2D, AvgPool2D</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> paddle.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> paddle.io <span class="keyword">import</span> Dataset, DataLoader, IterableDataset</span><br><span class="line"><span class="keyword">from</span> sklearn.utils <span class="keyword">import</span> shuffle</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> paddle.vision.transforms <span class="keyword">import</span> Resize</span><br><span class="line">PLACE = paddle.CUDAPlace(<span class="number">0</span>)  <span class="comment"># 在gpu上训练</span></span><br></pre></td></tr></table></figure><h2 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h2><p>读取解压好的数据集,将图片名全部保存 train.csv(训练集)和 test.csv(测试集)中</p><p>处理完后,就可以定义数据集类并读取数据集了</p><p><code>in[4]</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 写入训练集csv</span></span><br><span class="line">list1=[]</span><br><span class="line"><span class="keyword">for</span> path <span class="keyword">in</span> os.listdir(<span class="string">&quot;train/1_no_phone&quot;</span>) :</span><br><span class="line">    <span class="keyword">if</span> path[-<span class="number">3</span>:]==<span class="string">&#x27;jpg&#x27;</span>:</span><br><span class="line">        k=[<span class="string">&quot;train/1_no_phone/&quot;</span>+path,<span class="number">1</span>]</span><br><span class="line">        list1.append(k)</span><br><span class="line"><span class="keyword">for</span> path <span class="keyword">in</span> os.listdir(<span class="string">&quot;train/0_phone&quot;</span>) :</span><br><span class="line">    <span class="keyword">if</span> path[-<span class="number">3</span>:]==<span class="string">&#x27;jpg&#x27;</span>:</span><br><span class="line">        k=[<span class="string">&quot;train/0_phone/&quot;</span>+path,<span class="number">0</span>]</span><br><span class="line">        list1.append(k)</span><br><span class="line">result_df = pd.DataFrame(list1)</span><br><span class="line">result_df.columns=[<span class="string">&#x27;image_id&#x27;</span>, <span class="string">&#x27;label&#x27;</span>]</span><br><span class="line">data = shuffle(result_df)</span><br><span class="line">data.to_csv(<span class="string">&#x27;train.csv&#x27;</span>, index=<span class="literal">False</span>, header=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p><code>in[5]</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 写入测试集csv</span></span><br><span class="line">list1=[]</span><br><span class="line"><span class="keyword">for</span> path <span class="keyword">in</span> os.listdir(<span class="string">&quot;test&quot;</span>) :</span><br><span class="line">    <span class="keyword">if</span> path[-<span class="number">3</span>:]==<span class="string">&#x27;jpg&#x27;</span>:</span><br><span class="line">        k=[<span class="string">&quot;test/&quot;</span>+path]</span><br><span class="line">        list1.append(k)</span><br><span class="line">result_df = pd.DataFrame(list1)</span><br><span class="line">result_df.columns=[<span class="string">&#x27;image_id&#x27;</span>]</span><br><span class="line">result_df.to_csv(<span class="string">&#x27;test.csv&#x27;</span>, index=<span class="literal">False</span>, header=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p><code>in[6]</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pic_list=pd.read_csv(<span class="string">&quot;train.csv&quot;</span>)</span><br><span class="line">pic_list=np.array(pic_list)</span><br><span class="line">train_list=pic_list[:<span class="built_in">int</span>(<span class="built_in">len</span>(pic_list)*<span class="number">0.8</span>)]</span><br><span class="line">test_list=pic_list[<span class="built_in">int</span>(<span class="built_in">len</span>(pic_list)*<span class="number">0.8</span>):]</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(train_list))</span><br><span class="line">train_list</span><br></pre></td></tr></table></figure><p><code>out[6]</code></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">3320</span><br><span class="line"></span><br><span class="line">array([[&#x27;train/1_no_phone/tWuCoxIkbAedBjParJZQGn9XVYLK16hS.jpg&#x27;, 1],</span><br><span class="line">       [&#x27;train/1_no_phone/rkX9Rj7YqENc85bFxdDaw14Is0yCeAnp.jpg&#x27;, 1],</span><br><span class="line">       [&#x27;train/0_phone/wfhSF1W5B46doJKEc7VTU2OxbCtRDAI9.jpg&#x27;, 0],</span><br><span class="line">       ...,</span><br><span class="line">       [&#x27;train/1_no_phone/uvHPyhakBK89o43fjJRXgZ2CIEpGzSAQ.jpg&#x27;, 1],</span><br><span class="line">       [&#x27;train/0_phone/rlw854paLXkqc3zhgo9N2idUSeyIPAWf.jpg&#x27;, 0],</span><br><span class="line">       [&#x27;train/0_phone/Wy59OfsandAo1lp2S7e3cEutPbi4zqYF.jpg&#x27;, 0]],</span><br><span class="line">      dtype=object)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="离线图像增广"><a href="#离线图像增广" class="headerlink" title="离线图像增广"></a>离线图像增广</h2><p>对图像进行 70%概率的随机水平翻转，并对翻转后的图像亮度、对比度和饱和度进行变化，将增广后的图像保存到 augment 文件夹，将图像数据追加到 train_list 后面，</p><p><code>in[7]</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle.vision.transforms <span class="keyword">as</span> T</span><br><span class="line"></span><br><span class="line">directory_name = <span class="string">&quot;augment/&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(train_list)):</span><br><span class="line">    img_path = <span class="string">&#x27;/&#x27;</span>.join(train_list[i][<span class="number">0</span>].split(<span class="string">&#x27;/&#x27;</span>)[:<span class="number">4</span>])</span><br><span class="line">    img_path[<span class="number">6</span>:-<span class="number">4</span>]</span><br><span class="line">    img = Image.<span class="built_in">open</span>(img_path)</span><br><span class="line">    img = T.RandomHorizontalFlip(<span class="number">0.7</span>)(img)</span><br><span class="line">    <span class="comment"># img.save(directory_name + &quot;/&quot; + img_path[6:-4] + &quot;-tramsforms1.jpg&quot;)</span></span><br><span class="line">    img = T.ColorJitter(<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.0</span>)(img)</span><br><span class="line">    img.save(directory_name + <span class="string">&quot;/&quot;</span> + img_path[<span class="number">6</span>:-<span class="number">4</span>] + <span class="string">&quot;-tramsforms.jpg&quot;</span>)</span><br></pre></td></tr></table></figure><p><code>in[8]</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 写入训练集csv</span></span><br><span class="line">list4=[]</span><br><span class="line"><span class="keyword">for</span> path <span class="keyword">in</span> os.listdir(<span class="string">&quot;augment/1_no_phone&quot;</span>) :</span><br><span class="line">    <span class="keyword">if</span> path[-<span class="number">3</span>:]==<span class="string">&#x27;jpg&#x27;</span>:</span><br><span class="line">        k=[<span class="string">&quot;augment/1_no_phone/&quot;</span>+path,<span class="number">1</span>]</span><br><span class="line">        list4.append(k)</span><br><span class="line"><span class="keyword">for</span> path <span class="keyword">in</span> os.listdir(<span class="string">&quot;augment/0_phone&quot;</span>) :</span><br><span class="line">    <span class="keyword">if</span> path[-<span class="number">3</span>:]==<span class="string">&#x27;jpg&#x27;</span>:</span><br><span class="line">        k=[<span class="string">&quot;augment/0_phone/&quot;</span>+path,<span class="number">0</span>]</span><br><span class="line">        list4.append(k)</span><br><span class="line">result_df = pd.DataFrame(list4)</span><br><span class="line">result_df.columns=[<span class="string">&#x27;image_id&#x27;</span>, <span class="string">&#x27;label&#x27;</span>]</span><br><span class="line">data = shuffle(result_df)</span><br><span class="line">data.to_csv(<span class="string">&#x27;aug.csv&#x27;</span>, index=<span class="literal">False</span>, header=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p><code>in[9]</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">aug_list=pd.read_csv(<span class="string">&quot;aug.csv&quot;</span>)</span><br><span class="line">aug_list=np.array(aug_list)</span><br><span class="line">train_list = np.append(train_list, aug_list, axis=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(train_list))</span><br><span class="line">train_list</span><br></pre></td></tr></table></figure><p><code>out[9]</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">6640</span><br><span class="line"></span><br><span class="line">array([[&#x27;train/1_no_phone/tWuCoxIkbAedBjParJZQGn9XVYLK16hS.jpg&#x27;, 1],</span><br><span class="line">       [&#x27;train/1_no_phone/rkX9Rj7YqENc85bFxdDaw14Is0yCeAnp.jpg&#x27;, 1],</span><br><span class="line">       [&#x27;train/0_phone/wfhSF1W5B46doJKEc7VTU2OxbCtRDAI9.jpg&#x27;, 0],</span><br><span class="line">       ...,</span><br><span class="line">       [&#x27;augment/1_no_phone/WPNLg72lV8Jhn5TpokKMf34QR1Fd6IcH-tramsforms.jpg&#x27;,</span><br><span class="line">        1],</span><br><span class="line">       [&#x27;augment/0_phone/qJcVtZH2yiAL6w34rOQopT9IemzEDjWK-tramsforms.jpg&#x27;,</span><br><span class="line">        0],</span><br><span class="line">       [&#x27;augment/0_phone/qwoDX3ENaVuCk9IYjAnBHbv7TLUPR5Kr-tramsforms.jpg&#x27;,</span><br><span class="line">        0]], dtype=object)</span><br></pre></td></tr></table></figure><p><code>in[10]</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">H2ZDateset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data_dir</span>):</span><br><span class="line">        <span class="built_in">super</span>(H2ZDateset, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.pic_list=data_dir</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        image_file,label=<span class="variable language_">self</span>.pic_list[idx]</span><br><span class="line">        img = Image.<span class="built_in">open</span>(image_file)  <span class="comment"># 读取图片</span></span><br><span class="line">        img = img.resize((<span class="number">256</span>, <span class="number">256</span>), Image.ANTIALIAS)  <span class="comment"># 图片大小样式归一化</span></span><br><span class="line">        img = np.array(img).astype(<span class="string">&#x27;float32&#x27;</span>)  <span class="comment"># 转换成数组类型浮点型32位</span></span><br><span class="line">        img = img.transpose((<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">        img = img/<span class="number">255.0</span>  <span class="comment"># 数据缩放到0-1的范围</span></span><br><span class="line">        <span class="keyword">return</span> img, np.array(label, dtype=<span class="string">&#x27;int64&#x27;</span>).reshape(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.pic_list)</span><br></pre></td></tr></table></figure><p><code>in[11]</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">h2zdateset = H2ZDateset(train_list)</span><br><span class="line">BATCH_SIZE = <span class="number">32</span></span><br><span class="line">loader = DataLoader(h2zdateset, places=PLACE, shuffle=<span class="literal">True</span>, batch_size=BATCH_SIZE, drop_last=<span class="literal">False</span>, num_workers=<span class="number">0</span>, use_shared_memory=<span class="literal">False</span>)</span><br><span class="line">data,label = <span class="built_in">next</span>(loader())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;读取的数据形状：&quot;</span>, data.shape,label.shape)</span><br></pre></td></tr></table></figure><p><code>out[11]</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">读取的数据形状： [32, 3, 256, 256] [32, 1]</span><br></pre></td></tr></table></figure><p><code>in[12]</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data = H2ZDateset(train_list)</span><br><span class="line">test_data = H2ZDateset(test_list)</span><br><span class="line">train_data_reader = DataLoader(train_data, places=PLACE, shuffle=<span class="literal">True</span>, batch_size=BATCH_SIZE, drop_last=<span class="literal">False</span>, num_workers=<span class="number">2</span>, use_shared_memory=<span class="literal">True</span>)</span><br><span class="line">test_data_reader = DataLoader(test_data, places=PLACE, shuffle=<span class="literal">True</span>, batch_size=BATCH_SIZE, drop_last=<span class="literal">False</span>, num_workers=<span class="number">2</span>, use_shared_memory=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>首先进行 LeNet 模型组网</p><p>这里用到了 paddle.Model 的 summary()方法来将模型可视化,通过 summary()可以快速打印模型的网络结构，并且，执行该语句的时候会执行一次网络。在动态图中，我们需要手算网络的输入和输出层，如果出现一点问题就会报错非常麻烦，而 summary()能大大缩短 debug 时间</p><p>自定义 ResNeXt 类，导入 resnext101_64x4d 网络，并开启预训练模型的选项，增一个线性层，将原本模型的 1000 分类问题变成 2 分类问题</p><p><code>in[13]</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResNeXt</span>(nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(ResNeXt, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.layer = paddle.vision.models.resnext101_64x4d(pretrained=<span class="literal">True</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Sequential(</span><br><span class="line">            nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">            nn.Linear(<span class="number">1000</span>, <span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        outputs = <span class="variable language_">self</span>.layer(inputs)</span><br><span class="line">        outputs = <span class="variable language_">self</span>.fc(outputs)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">model = ResNeXt()</span><br><span class="line">paddle.Model(model).summary((-<span class="number">1</span>, <span class="number">3</span>, <span class="number">256</span>, <span class="number">256</span>))</span><br></pre></td></tr></table></figure><p><code>out[13]</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">W0510 00:32:22.100802  8743 gpu_context.cc:244] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 10.1</span><br><span class="line">W0510 00:32:22.104816  8743 gpu_context.cc:272] device: 0, cuDNN Version: 7.6.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">-------------------------------------------------------------------------------</span><br><span class="line">  Layer (type)         Input Shape          Output Shape         Param #</span><br><span class="line">===============================================================================</span><br><span class="line">    Conv2D-1        [[1, 3, 256, 256]]   [1, 64, 128, 128]        9,408</span><br><span class="line">    BatchNorm-1     [[1, 64, 128, 128]]   [1, 64, 128, 128]         256</span><br><span class="line">  ConvBNLayer-1     [[1, 3, 256, 256]]   [1, 64, 128, 128]          0</span><br><span class="line">    MaxPool2D-1     [[1, 64, 128, 128]]    [1, 64, 64, 64]           0</span><br><span class="line">    Conv2D-2        [[1, 64, 64, 64]]     [1, 256, 64, 64]       16,384</span><br><span class="line">    BatchNorm-2      [[1, 256, 64, 64]]    [1, 256, 64, 64]        1,024</span><br><span class="line">  ConvBNLayer-2     [[1, 64, 64, 64]]     [1, 256, 64, 64]          0</span><br><span class="line">    Conv2D-3        [[1, 256, 64, 64]]    [1, 256, 64, 64]        9,216</span><br><span class="line">    BatchNorm-3      [[1, 256, 64, 64]]    [1, 256, 64, 64]        1,024</span><br><span class="line">  ConvBNLayer-3     [[1, 256, 64, 64]]    [1, 256, 64, 64]          0</span><br><span class="line">    Conv2D-4        [[1, 256, 64, 64]]    [1, 256, 64, 64]       65,536</span><br><span class="line">    BatchNorm-4      [[1, 256, 64, 64]]    [1, 256, 64, 64]        1,024</span><br><span class="line">  ConvBNLayer-4     [[1, 256, 64, 64]]    [1, 256, 64, 64]          0</span><br><span class="line">    Conv2D-5        [[1, 64, 64, 64]]     [1, 256, 64, 64]       16,384</span><br><span class="line">    BatchNorm-5      [[1, 256, 64, 64]]    [1, 256, 64, 64]        1,024</span><br><span class="line">  ConvBNLayer-5     [[1, 64, 64, 64]]     [1, 256, 64, 64]          0</span><br><span class="line">BottleneckBlock-1   [[1, 64, 64, 64]]     [1, 256, 64, 64]          0</span><br><span class="line">    Conv2D-6        [[1, 256, 64, 64]]    [1, 256, 64, 64]       65,536</span><br><span class="line">    BatchNorm-6      [[1, 256, 64, 64]]    [1, 256, 64, 64]        1,024</span><br><span class="line">  ConvBNLayer-6     [[1, 256, 64, 64]]    [1, 256, 64, 64]          0</span><br><span class="line">    Conv2D-7        [[1, 256, 64, 64]]    [1, 256, 64, 64]        9,216</span><br><span class="line">    BatchNorm-7      [[1, 256, 64, 64]]    [1, 256, 64, 64]        1,024</span><br><span class="line">  ConvBNLayer-7     [[1, 256, 64, 64]]    [1, 256, 64, 64]          0</span><br><span class="line">    Conv2D-8        [[1, 256, 64, 64]]    [1, 256, 64, 64]       65,536</span><br><span class="line">    BatchNorm-8      [[1, 256, 64, 64]]    [1, 256, 64, 64]        1,024</span><br><span class="line">  ConvBNLayer-8     [[1, 256, 64, 64]]    [1, 256, 64, 64]          0</span><br><span class="line">BottleneckBlock-2   [[1, 256, 64, 64]]    [1, 256, 64, 64]          0</span><br><span class="line">    Conv2D-9        [[1, 256, 64, 64]]    [1, 256, 64, 64]       65,536</span><br><span class="line">    BatchNorm-9      [[1, 256, 64, 64]]    [1, 256, 64, 64]        1,024</span><br><span class="line">  ConvBNLayer-9     [[1, 256, 64, 64]]    [1, 256, 64, 64]          0</span><br><span class="line">    Conv2D-10       [[1, 256, 64, 64]]    [1, 256, 64, 64]        9,216</span><br><span class="line">  BatchNorm-10      [[1, 256, 64, 64]]    [1, 256, 64, 64]        1,024</span><br><span class="line">  ConvBNLayer-10     [[1, 256, 64, 64]]    [1, 256, 64, 64]          0</span><br><span class="line">    Conv2D-11       [[1, 256, 64, 64]]    [1, 256, 64, 64]       65,536</span><br><span class="line">  BatchNorm-11      [[1, 256, 64, 64]]    [1, 256, 64, 64]        1,024</span><br><span class="line">  ConvBNLayer-11     [[1, 256, 64, 64]]    [1, 256, 64, 64]          0</span><br><span class="line">BottleneckBlock-3   [[1, 256, 64, 64]]    [1, 256, 64, 64]          0</span><br><span class="line">    Conv2D-12       [[1, 256, 64, 64]]    [1, 512, 64, 64]       131,072</span><br><span class="line">  BatchNorm-12      [[1, 512, 64, 64]]    [1, 512, 64, 64]        2,048</span><br><span class="line">  ConvBNLayer-12     [[1, 256, 64, 64]]    [1, 512, 64, 64]          0</span><br><span class="line">    Conv2D-13       [[1, 512, 64, 64]]    [1, 512, 32, 32]       36,864</span><br><span class="line">  BatchNorm-13      [[1, 512, 32, 32]]    [1, 512, 32, 32]        2,048</span><br><span class="line">  ConvBNLayer-13     [[1, 512, 64, 64]]    [1, 512, 32, 32]          0</span><br><span class="line">    Conv2D-14       [[1, 512, 32, 32]]    [1, 512, 32, 32]       262,144</span><br><span class="line">  BatchNorm-14      [[1, 512, 32, 32]]    [1, 512, 32, 32]        2,048</span><br><span class="line">  ConvBNLayer-14     [[1, 512, 32, 32]]    [1, 512, 32, 32]          0</span><br><span class="line">    Conv2D-15       [[1, 256, 64, 64]]    [1, 512, 32, 32]       131,072</span><br><span class="line">  BatchNorm-15      [[1, 512, 32, 32]]    [1, 512, 32, 32]        2,048</span><br><span class="line">  ConvBNLayer-15     [[1, 256, 64, 64]]    [1, 512, 32, 32]          0</span><br><span class="line">BottleneckBlock-4   [[1, 256, 64, 64]]    [1, 512, 32, 32]          0</span><br><span class="line">    Conv2D-16       [[1, 512, 32, 32]]    [1, 512, 32, 32]       262,144</span><br><span class="line">  BatchNorm-16      [[1, 512, 32, 32]]    [1, 512, 32, 32]        2,048</span><br><span class="line">  ConvBNLayer-16     [[1, 512, 32, 32]]    [1, 512, 32, 32]          0</span><br><span class="line">    Conv2D-17       [[1, 512, 32, 32]]    [1, 512, 32, 32]       36,864</span><br><span class="line">  BatchNorm-17      [[1, 512, 32, 32]]    [1, 512, 32, 32]        2,048</span><br><span class="line">  ConvBNLayer-17     [[1, 512, 32, 32]]    [1, 512, 32, 32]          0</span><br><span class="line">    Conv2D-18       [[1, 512, 32, 32]]    [1, 512, 32, 32]       262,144</span><br><span class="line">  BatchNorm-18      [[1, 512, 32, 32]]    [1, 512, 32, 32]        2,048</span><br><span class="line">  ConvBNLayer-18     [[1, 512, 32, 32]]    [1, 512, 32, 32]          0</span><br><span class="line">BottleneckBlock-5   [[1, 512, 32, 32]]    [1, 512, 32, 32]          0</span><br><span class="line">    Conv2D-19       [[1, 512, 32, 32]]    [1, 512, 32, 32]       262,144</span><br><span class="line">  BatchNorm-19      [[1, 512, 32, 32]]    [1, 512, 32, 32]        2,048</span><br><span class="line">  ConvBNLayer-19     [[1, 512, 32, 32]]    [1, 512, 32, 32]          0</span><br><span class="line">    Conv2D-20       [[1, 512, 32, 32]]    [1, 512, 32, 32]       36,864</span><br><span class="line">  BatchNorm-20      [[1, 512, 32, 32]]    [1, 512, 32, 32]        2,048</span><br><span class="line">  ConvBNLayer-20     [[1, 512, 32, 32]]    [1, 512, 32, 32]          0</span><br><span class="line">    Conv2D-21       [[1, 512, 32, 32]]    [1, 512, 32, 32]       262,144</span><br><span class="line">  BatchNorm-21      [[1, 512, 32, 32]]    [1, 512, 32, 32]        2,048</span><br><span class="line">  ConvBNLayer-21     [[1, 512, 32, 32]]    [1, 512, 32, 32]          0</span><br><span class="line">BottleneckBlock-6   [[1, 512, 32, 32]]    [1, 512, 32, 32]          0</span><br><span class="line">    Conv2D-22       [[1, 512, 32, 32]]    [1, 512, 32, 32]       262,144</span><br><span class="line">  BatchNorm-22      [[1, 512, 32, 32]]    [1, 512, 32, 32]        2,048</span><br><span class="line">  ConvBNLayer-22     [[1, 512, 32, 32]]    [1, 512, 32, 32]          0</span><br><span class="line">    Conv2D-23       [[1, 512, 32, 32]]    [1, 512, 32, 32]       36,864</span><br><span class="line">  BatchNorm-23      [[1, 512, 32, 32]]    [1, 512, 32, 32]        2,048</span><br><span class="line">  ConvBNLayer-23     [[1, 512, 32, 32]]    [1, 512, 32, 32]          0</span><br><span class="line">    Conv2D-24       [[1, 512, 32, 32]]    [1, 512, 32, 32]       262,144</span><br><span class="line">  BatchNorm-24      [[1, 512, 32, 32]]    [1, 512, 32, 32]        2,048</span><br><span class="line">  ConvBNLayer-24     [[1, 512, 32, 32]]    [1, 512, 32, 32]          0</span><br><span class="line">BottleneckBlock-7   [[1, 512, 32, 32]]    [1, 512, 32, 32]          0</span><br><span class="line">    Conv2D-25       [[1, 512, 32, 32]]   [1, 1024, 32, 32]       524,288</span><br><span class="line">  BatchNorm-25     [[1, 1024, 32, 32]]   [1, 1024, 32, 32]        4,096</span><br><span class="line">  ConvBNLayer-25     [[1, 512, 32, 32]]   [1, 1024, 32, 32]          0</span><br><span class="line">    Conv2D-26      [[1, 1024, 32, 32]]   [1, 1024, 16, 16]       147,456</span><br><span class="line">  BatchNorm-26     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-26    [[1, 1024, 32, 32]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-27      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-27     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-27    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-28       [[1, 512, 32, 32]]   [1, 1024, 16, 16]       524,288</span><br><span class="line">  BatchNorm-28     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-28     [[1, 512, 32, 32]]   [1, 1024, 16, 16]          0</span><br><span class="line">BottleneckBlock-8   [[1, 512, 32, 32]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-29      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-29     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-29    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-30      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]       147,456</span><br><span class="line">  BatchNorm-30     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-30    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-31      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-31     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-31    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">BottleneckBlock-9  [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-32      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-32     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-32    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-33      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]       147,456</span><br><span class="line">  BatchNorm-33     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-33    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-34      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-34     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-34    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">BottleneckBlock-10  [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-35      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-35     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-35    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-36      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]       147,456</span><br><span class="line">  BatchNorm-36     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-36    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-37      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-37     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-37    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">BottleneckBlock-11  [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-38      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-38     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-38    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-39      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]       147,456</span><br><span class="line">  BatchNorm-39     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-39    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-40      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-40     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-40    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">BottleneckBlock-12  [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-41      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-41     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-41    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-42      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]       147,456</span><br><span class="line">  BatchNorm-42     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-42    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-43      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-43     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-43    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">BottleneckBlock-13  [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-44      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-44     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-44    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-45      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]       147,456</span><br><span class="line">  BatchNorm-45     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-45    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-46      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-46     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-46    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">BottleneckBlock-14  [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-47      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-47     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-47    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-48      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]       147,456</span><br><span class="line">  BatchNorm-48     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-48    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-49      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-49     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-49    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">BottleneckBlock-15  [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-50      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-50     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-50    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-51      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]       147,456</span><br><span class="line">  BatchNorm-51     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-51    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-52      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-52     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-52    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">BottleneckBlock-16  [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-53      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-53     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-53    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-54      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]       147,456</span><br><span class="line">  BatchNorm-54     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-54    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-55      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-55     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-55    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">BottleneckBlock-17  [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-56      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-56     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-56    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-57      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]       147,456</span><br><span class="line">  BatchNorm-57     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-57    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-58      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-58     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-58    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">BottleneckBlock-18  [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-59      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-59     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-59    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-60      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]       147,456</span><br><span class="line">  BatchNorm-60     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-60    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-61      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-61     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-61    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">BottleneckBlock-19  [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-62      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-62     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-62    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-63      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]       147,456</span><br><span class="line">  BatchNorm-63     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-63    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-64      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-64     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-64    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">BottleneckBlock-20  [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-65      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-65     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-65    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-66      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]       147,456</span><br><span class="line">  BatchNorm-66     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-66    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-67      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-67     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-67    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">BottleneckBlock-21  [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-68      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-68     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-68    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-69      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]       147,456</span><br><span class="line">  BatchNorm-69     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-69    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-70      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-70     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-70    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">BottleneckBlock-22  [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-71      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-71     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-71    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-72      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]       147,456</span><br><span class="line">  BatchNorm-72     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-72    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-73      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-73     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-73    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">BottleneckBlock-23  [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-74      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-74     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-74    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-75      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]       147,456</span><br><span class="line">  BatchNorm-75     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-75    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-76      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-76     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-76    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">BottleneckBlock-24  [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-77      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-77     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-77    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-78      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]       147,456</span><br><span class="line">  BatchNorm-78     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-78    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-79      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-79     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-79    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">BottleneckBlock-25  [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-80      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-80     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-80    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-81      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]       147,456</span><br><span class="line">  BatchNorm-81     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-81    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-82      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-82     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-82    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">BottleneckBlock-26  [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-83      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-83     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-83    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-84      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]       147,456</span><br><span class="line">  BatchNorm-84     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-84    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-85      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-85     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-85    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">BottleneckBlock-27  [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-86      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-86     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-86    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-87      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]       147,456</span><br><span class="line">  BatchNorm-87     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-87    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-88      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-88     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-88    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">BottleneckBlock-28  [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-89      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-89     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-89    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-90      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]       147,456</span><br><span class="line">  BatchNorm-90     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-90    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-91      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-91     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-91    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">BottleneckBlock-29  [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-92      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-92     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-92    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-93      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]       147,456</span><br><span class="line">  BatchNorm-93     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-93    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-94      [[1, 1024, 16, 16]]   [1, 1024, 16, 16]      1,048,576</span><br><span class="line">  BatchNorm-94     [[1, 1024, 16, 16]]   [1, 1024, 16, 16]        4,096</span><br><span class="line">  ConvBNLayer-94    [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">BottleneckBlock-30  [[1, 1024, 16, 16]]   [1, 1024, 16, 16]          0</span><br><span class="line">    Conv2D-95      [[1, 1024, 16, 16]]   [1, 2048, 16, 16]      2,097,152</span><br><span class="line">  BatchNorm-95     [[1, 2048, 16, 16]]   [1, 2048, 16, 16]        8,192</span><br><span class="line">  ConvBNLayer-95    [[1, 1024, 16, 16]]   [1, 2048, 16, 16]          0</span><br><span class="line">    Conv2D-96      [[1, 2048, 16, 16]]    [1, 2048, 8, 8]        589,824</span><br><span class="line">  BatchNorm-96      [[1, 2048, 8, 8]]     [1, 2048, 8, 8]         8,192</span><br><span class="line">  ConvBNLayer-96    [[1, 2048, 16, 16]]    [1, 2048, 8, 8]           0</span><br><span class="line">    Conv2D-97       [[1, 2048, 8, 8]]     [1, 2048, 8, 8]       4,194,304</span><br><span class="line">  BatchNorm-97      [[1, 2048, 8, 8]]     [1, 2048, 8, 8]         8,192</span><br><span class="line">  ConvBNLayer-97     [[1, 2048, 8, 8]]     [1, 2048, 8, 8]           0</span><br><span class="line">    Conv2D-98      [[1, 1024, 16, 16]]    [1, 2048, 8, 8]       2,097,152</span><br><span class="line">  BatchNorm-98      [[1, 2048, 8, 8]]     [1, 2048, 8, 8]         8,192</span><br><span class="line">  ConvBNLayer-98    [[1, 1024, 16, 16]]    [1, 2048, 8, 8]           0</span><br><span class="line">BottleneckBlock-31  [[1, 1024, 16, 16]]    [1, 2048, 8, 8]           0</span><br><span class="line">    Conv2D-99       [[1, 2048, 8, 8]]     [1, 2048, 8, 8]       4,194,304</span><br><span class="line">  BatchNorm-99      [[1, 2048, 8, 8]]     [1, 2048, 8, 8]         8,192</span><br><span class="line">  ConvBNLayer-99     [[1, 2048, 8, 8]]     [1, 2048, 8, 8]           0</span><br><span class="line">    Conv2D-100       [[1, 2048, 8, 8]]     [1, 2048, 8, 8]        589,824</span><br><span class="line">  BatchNorm-100     [[1, 2048, 8, 8]]     [1, 2048, 8, 8]         8,192</span><br><span class="line">  ConvBNLayer-100    [[1, 2048, 8, 8]]     [1, 2048, 8, 8]           0</span><br><span class="line">    Conv2D-101       [[1, 2048, 8, 8]]     [1, 2048, 8, 8]       4,194,304</span><br><span class="line">  BatchNorm-101     [[1, 2048, 8, 8]]     [1, 2048, 8, 8]         8,192</span><br><span class="line">  ConvBNLayer-101    [[1, 2048, 8, 8]]     [1, 2048, 8, 8]           0</span><br><span class="line">BottleneckBlock-32   [[1, 2048, 8, 8]]     [1, 2048, 8, 8]           0</span><br><span class="line">    Conv2D-102       [[1, 2048, 8, 8]]     [1, 2048, 8, 8]       4,194,304</span><br><span class="line">  BatchNorm-102     [[1, 2048, 8, 8]]     [1, 2048, 8, 8]         8,192</span><br><span class="line">  ConvBNLayer-102    [[1, 2048, 8, 8]]     [1, 2048, 8, 8]           0</span><br><span class="line">    Conv2D-103       [[1, 2048, 8, 8]]     [1, 2048, 8, 8]        589,824</span><br><span class="line">  BatchNorm-103     [[1, 2048, 8, 8]]     [1, 2048, 8, 8]         8,192</span><br><span class="line">  ConvBNLayer-103    [[1, 2048, 8, 8]]     [1, 2048, 8, 8]           0</span><br><span class="line">    Conv2D-104       [[1, 2048, 8, 8]]     [1, 2048, 8, 8]       4,194,304</span><br><span class="line">  BatchNorm-104     [[1, 2048, 8, 8]]     [1, 2048, 8, 8]         8,192</span><br><span class="line">  ConvBNLayer-104    [[1, 2048, 8, 8]]     [1, 2048, 8, 8]           0</span><br><span class="line">BottleneckBlock-33   [[1, 2048, 8, 8]]     [1, 2048, 8, 8]           0</span><br><span class="line">AdaptiveAvgPool2D-1  [[1, 2048, 8, 8]]     [1, 2048, 1, 1]           0</span><br><span class="line">    Linear-1           [[1, 2048]]           [1, 1000]          2,049,000</span><br><span class="line">    ResNeXt-2       [[1, 3, 256, 256]]       [1, 1000]              0</span><br><span class="line">    Dropout-1          [[1, 1000]]           [1, 1000]              0</span><br><span class="line">    Linear-2           [[1, 1000]]             [1, 2]             2,002</span><br><span class="line">===============================================================================</span><br><span class="line">Total params: 83,660,154</span><br><span class="line">Trainable params: 83,254,394</span><br><span class="line">Non-trainable params: 405,760</span><br><span class="line">-------------------------------------------------------------------------------</span><br><span class="line">Input size (MB): 0.75</span><br><span class="line">Forward/backward pass size (MB): 1024.04</span><br><span class="line">Params size (MB): 319.14</span><br><span class="line">Estimated Total Size (MB): 1343.93</span><br><span class="line">-------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">&#123;&#x27;total_params&#x27;: 83660154, &#x27;trainable_params&#x27;: 83254394&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h2><p>模型训练好之后就可以开始预测</p><p>在使用带有动量的 SGD 学习率优化器后训练 5 个 epoch，在 valid_list 中已经可以达到 95.3%的准确率</p><p><code>in[14]</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">epochs_num = <span class="number">5</span> <span class="comment">#迭代次数</span></span><br><span class="line"><span class="comment"># opt = paddle.optimizer.Adam(learning_rate=0.001, parameters=model.parameters())</span></span><br><span class="line">opt = paddle.optimizer.Momentum(learning_rate=<span class="number">0.002</span>, parameters=model.parameters(), weight_decay=<span class="number">0.005</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">train_acc, train_loss, valid_acc = [], [], []</span><br><span class="line"><span class="keyword">for</span> pass_num <span class="keyword">in</span> <span class="built_in">range</span>(epochs_num):</span><br><span class="line">    model.train() <span class="comment">#训练模式</span></span><br><span class="line">    accs=[]</span><br><span class="line">    <span class="keyword">for</span> batch_id,data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_data_reader):</span><br><span class="line">        images, labels = data</span><br><span class="line">        predict = model(images)<span class="comment">#预测</span></span><br><span class="line">        loss=F.cross_entropy(predict,labels)</span><br><span class="line">        avg_loss=paddle.mean(loss)</span><br><span class="line">        acc=paddle.metric.accuracy(predict,labels)<span class="comment">#计算精度</span></span><br><span class="line">        accs.append(acc.numpy()[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">if</span> batch_id % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;epoch:&#123;&#125;, iter:&#123;&#125;, loss:&#123;&#125;, acc:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(pass_num, batch_id, avg_loss.numpy(), acc.numpy()[<span class="number">0</span>]))</span><br><span class="line">        opt.clear_grad()</span><br><span class="line">        avg_loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;train_pass:&#123;&#125;, train_loss:&#123;&#125;, train_acc:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(pass_num, avg_loss.numpy(), np.mean(accs)))</span><br><span class="line">    train_acc.append(np.mean(accs)), train_loss.append(avg_loss.numpy())</span><br><span class="line">    <span class="comment">##得到验证集的性能</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    val_accs=[]</span><br><span class="line">    <span class="keyword">for</span> batch_id,data <span class="keyword">in</span> <span class="built_in">enumerate</span>(test_data_reader):</span><br><span class="line">            images, labels = data</span><br><span class="line">            predict=model(images)<span class="comment">#预测</span></span><br><span class="line">            acc=paddle.metric.accuracy(predict,labels)<span class="comment">#计算精度</span></span><br><span class="line">            val_accs.append(acc.numpy()[<span class="number">0</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;val_acc=&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(np.mean(val_accs)))</span><br><span class="line">    valid_acc.append(np.mean(val_accs))</span><br><span class="line">paddle.save(model.state_dict(),<span class="string">&#x27;resnext101_64x4d&#x27;</span>)<span class="comment">#保存模型</span></span><br></pre></td></tr></table></figure><p><code>out[14]</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">epoch:0, iter:0, loss:[1.2695745], acc:0.5625</span><br><span class="line">epoch:0, iter:20, loss:[0.72218126], acc:0.78125</span><br><span class="line">epoch:0, iter:40, loss:[0.8521573], acc:0.625</span><br><span class="line">epoch:0, iter:60, loss:[0.9325488], acc:0.78125</span><br><span class="line">epoch:0, iter:80, loss:[0.43677044], acc:0.90625</span><br><span class="line">epoch:0, iter:100, loss:[0.37998652], acc:0.84375</span><br><span class="line">epoch:0, iter:120, loss:[0.17167747], acc:0.9375</span><br><span class="line">epoch:0, iter:140, loss:[0.13679577], acc:0.9375</span><br><span class="line">epoch:0, iter:160, loss:[0.27065688], acc:0.9375</span><br><span class="line">epoch:0, iter:180, loss:[0.27734363], acc:0.875</span><br><span class="line">epoch:0, iter:200, loss:[0.27193183], acc:0.90625</span><br><span class="line">train_pass:0, train_loss:[0.37479421], train_acc:0.8610276579856873</span><br><span class="line">val_acc=0.9120657444000244</span><br><span class="line">epoch:1, iter:0, loss:[0.29650056], acc:0.875</span><br><span class="line">epoch:1, iter:20, loss:[0.03968127], acc:0.96875</span><br><span class="line">epoch:1, iter:40, loss:[0.0579604], acc:0.96875</span><br><span class="line">epoch:1, iter:60, loss:[0.08308301], acc:0.96875</span><br><span class="line">epoch:1, iter:80, loss:[0.02734788], acc:1.0</span><br><span class="line">epoch:1, iter:100, loss:[0.04930278], acc:0.96875</span><br><span class="line">epoch:1, iter:120, loss:[0.08352702], acc:0.96875</span><br><span class="line">epoch:1, iter:140, loss:[0.180908], acc:0.96875</span><br><span class="line">epoch:1, iter:160, loss:[0.00271969], acc:1.0</span><br><span class="line">epoch:1, iter:180, loss:[0.23547903], acc:0.90625</span><br><span class="line">epoch:1, iter:200, loss:[0.02033656], acc:1.0</span><br><span class="line">train_pass:1, train_loss:[0.00160835], train_acc:0.9695011973381042</span><br><span class="line">val_acc=0.9362205266952515</span><br><span class="line">epoch:2, iter:0, loss:[0.06544636], acc:0.96875</span><br><span class="line">epoch:2, iter:20, loss:[0.06581119], acc:0.96875</span><br><span class="line">epoch:2, iter:40, loss:[0.00562927], acc:1.0</span><br><span class="line">epoch:2, iter:60, loss:[0.01040748], acc:1.0</span><br><span class="line">epoch:2, iter:80, loss:[0.03810382], acc:0.96875</span><br><span class="line">epoch:2, iter:100, loss:[0.01447718], acc:1.0</span><br><span class="line">epoch:2, iter:120, loss:[0.12421186], acc:0.96875</span><br><span class="line">epoch:2, iter:140, loss:[0.00112416], acc:1.0</span><br><span class="line">epoch:2, iter:160, loss:[0.003324], acc:1.0</span><br><span class="line">epoch:2, iter:180, loss:[0.01755645], acc:1.0</span><br><span class="line">epoch:2, iter:200, loss:[0.06159591], acc:0.96875</span><br><span class="line">train_pass:2, train_loss:[0.00045859], train_acc:0.990234375</span><br><span class="line">val_acc=0.9543269276618958</span><br><span class="line">epoch:3, iter:0, loss:[0.00377628], acc:1.0</span><br><span class="line">epoch:3, iter:20, loss:[0.0032537], acc:1.0</span><br><span class="line">epoch:3, iter:40, loss:[0.00095566], acc:1.0</span><br><span class="line">epoch:3, iter:60, loss:[0.1955388], acc:0.9375</span><br><span class="line">epoch:3, iter:80, loss:[0.00345089], acc:1.0</span><br><span class="line">epoch:3, iter:100, loss:[0.00279539], acc:1.0</span><br><span class="line">epoch:3, iter:120, loss:[0.01505984], acc:1.0</span><br><span class="line">epoch:3, iter:140, loss:[0.06922489], acc:0.96875</span><br><span class="line">epoch:3, iter:160, loss:[0.05011526], acc:0.96875</span><br><span class="line">epoch:3, iter:180, loss:[0.13640904], acc:0.96875</span><br><span class="line">epoch:3, iter:200, loss:[0.01501912], acc:1.0</span><br><span class="line">train_pass:3, train_loss:[0.00018014], train_acc:0.991135835647583</span><br><span class="line">val_acc=0.944672703742981</span><br><span class="line">epoch:4, iter:0, loss:[0.00011357], acc:1.0</span><br><span class="line">epoch:4, iter:20, loss:[0.00145513], acc:1.0</span><br><span class="line">epoch:4, iter:40, loss:[0.00594005], acc:1.0</span><br><span class="line">epoch:4, iter:60, loss:[0.00403259], acc:1.0</span><br><span class="line">epoch:4, iter:80, loss:[0.00525207], acc:1.0</span><br><span class="line">epoch:4, iter:100, loss:[0.01096269], acc:1.0</span><br><span class="line">epoch:4, iter:120, loss:[0.0166808], acc:1.0</span><br><span class="line">epoch:4, iter:140, loss:[0.00298717], acc:1.0</span><br><span class="line">epoch:4, iter:160, loss:[0.00306062], acc:1.0</span><br><span class="line">epoch:4, iter:180, loss:[0.0080948], acc:1.0</span><br><span class="line">epoch:4, iter:200, loss:[0.08889838], acc:0.9375</span><br><span class="line">train_pass:4, train_loss:[0.07838897], train_acc:0.9887319803237915</span><br><span class="line">val_acc=0.9530086517333984</span><br></pre></td></tr></table></figure><p><code>in[15]</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line">test=pd.read_csv(<span class="string">&quot;test.csv&quot;</span>)</span><br><span class="line">test[<span class="string">&quot;label&quot;</span>]=<span class="number">0</span></span><br><span class="line"><span class="keyword">with</span> tqdm(total=<span class="built_in">len</span>(test)) <span class="keyword">as</span> pbar:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(test)):</span><br><span class="line">        image_file=test[<span class="string">&quot;image_id&quot;</span>][i]</span><br><span class="line">        pbar.set_description(<span class="string">f&#x27;Processing: <span class="subst">&#123;test[<span class="string">&quot;image_id&quot;</span>][i]&#125;</span>&#x27;</span>)</span><br><span class="line">        img = Image.<span class="built_in">open</span>(image_file)  <span class="comment"># 读取图片</span></span><br><span class="line">        img = img.resize((<span class="number">256</span>, <span class="number">256</span>), Image.ANTIALIAS)  <span class="comment"># 图片大小样式归一化</span></span><br><span class="line">        img = np.array(img).astype(<span class="string">&#x27;float32&#x27;</span>)  <span class="comment"># 转换成数组类型浮点型32位</span></span><br><span class="line">        img = img.transpose((<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">        img = img/<span class="number">255.0</span>  <span class="comment"># 数据缩放到0-1的范围</span></span><br><span class="line">        img=paddle.to_tensor(img.reshape(-<span class="number">1</span>,<span class="number">3</span>,<span class="number">256</span>,<span class="number">256</span>))</span><br><span class="line">        predict = model(img)</span><br><span class="line">        result = <span class="built_in">int</span>(np.argmax(predict.numpy()))</span><br><span class="line">        test.loc[i, <span class="string">&quot;image_id&quot;</span>]=test.loc[i, <span class="string">&quot;image_id&quot;</span>][<span class="number">5</span>:]</span><br><span class="line">        test.loc[i, <span class="string">&quot;label&quot;</span>]=result</span><br><span class="line">        pbar.update(<span class="number">1</span>)</span><br><span class="line">test.to_csv(<span class="string">&#x27;predict_result.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;end&quot;</span>)</span><br></pre></td></tr></table></figure><p><code>out[15]</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Processing: test/yheWlrsgStOjfLdqHbYE7p1P28ViDBQ9.jpg: 100%|██████████| 1849/1849 [01:58&lt;00:00, 15.65it/s]</span><br><span class="line">end</span><br></pre></td></tr></table></figure><h2 id="改进方向"><a href="#改进方向" class="headerlink" title="改进方向"></a>改进方向</h2><ol><li>可以在基线模型的基础上通过调参及模型优化进一步提升效果</li><li>可以对训练集进行数据增强从而增大训练数据量以提升模型泛化能力</li><li>可以尝试采用更深的神经网络，如 Resnet、VGG</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> paddlepaddle </tag>
            
            <tag> 飞桨 </tag>
            
            <tag> 图像分类 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>解决PaddlePaddle飞桨在迁移学习使用预训练模型时更改num_classes参数出现警告</title>
      <link href="/p/2023/PaddleTransferLearningFixUserWarning/"/>
      <url>/p/2023/PaddleTransferLearningFixUserWarning/</url>
      
        <content type="html"><![CDATA[<p>当我们使用 PaddlePaddle 进行迁移学习的时候，直接导入模型虽然是可以的，但是总是会有个警告</p><p>如直接用官方的 <code>resnet101</code> 并加载预训练模型的话</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = paddle.vision.models.resnet101(pretrained=<span class="literal">True</span>, num_classes=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>会提示这些信息：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">model = paddle.vision.models.resnet101(pretrained=True, num_classes=2)</span><br><span class="line">W0508 14:42:41.530314  1313 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 10.1</span><br><span class="line">W0508 14:42:41.535259  1313 device_context.cc:465] device: 0, cuDNN Version: 7.6.</span><br><span class="line">/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1441: UserWarning: Skip loading for fc.weight. fc.weight receives a shape [2048, 1000], but the expected shape is [2048, 2].</span><br><span class="line">  warnings.warn((&quot;Skip loading for &#123;&#125;. &quot;.format(key) + str(err)))</span><br><span class="line">/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1441: UserWarning: Skip loading for fc.bias. fc.bias receives a shape [1000], but the expected shape is [2].</span><br><span class="line">  warnings.warn((&quot;Skip loading for &#123;&#125;. &quot;.format(key) + str(err)))</span><br></pre></td></tr></table></figure><p>虽然说最后的训练效果还是会很不错，但是这个警告信息看着却是很难受，原因是因为我们自己分类的数据集 <code>num_classes</code> 只有 <code>2</code> ，和预训练模型的 <code>num_classes</code> 并不匹配，因此我们要进行以下操作，实现既加载了预训练模型，又能更好的训练自己的数据</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResNet</span>(nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(ResNeXt, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 加载预训练模型</span></span><br><span class="line">        <span class="variable language_">self</span>.layer = paddle.vision.models.resnet101(pretrained=<span class="literal">True</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Sequential(</span><br><span class="line">            nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">            nn.Linear(<span class="number">1000</span>, <span class="number">100</span>),</span><br><span class="line">            nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">            nn.Linear(<span class="number">100</span>, <span class="number">2</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        outputs = <span class="variable language_">self</span>.layer(inputs)</span><br><span class="line">        outputs = <span class="variable language_">self</span>.fc(outputs)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">model = ResNet()</span><br></pre></td></tr></table></figure><p>经过此番操作之后，输出内容就没有警告了</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">W0510 00:19:38.900521  7391 gpu_context.cc:244] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 10.1</span><br><span class="line">W0510 00:19:38.904939  7391 gpu_context.cc:272] device: 0, cuDNN Version: 7.6.</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> paddlepaddle </tag>
            
            <tag> 飞桨 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PaddlePaddle 自动求导</title>
      <link href="/p/2023/PaddlePaddleAutoGrad/"/>
      <url>/p/2023/PaddlePaddleAutoGrad/</url>
      
        <content type="html"><![CDATA[<h2 id="自动求导"><a href="#自动求导" class="headerlink" title="自动求导"></a>自动求导</h2><p>在 PaddlePaddle 中使用自动求导来计算导数。</p><p>要求：$ f(x)=\sin{x} $,绘制 $f(x)$ 和 $\dfrac{\mathrm{d}f(x)}{\mathrm{d}x}$ 的图像，不能使用 $ f’(x)=\cos{x}$</p><p><code>in[1]</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">x = paddle.to_tensor(np.arange(-<span class="number">5</span>, <span class="number">5</span>, <span class="number">0.01</span>), dtype=<span class="string">&quot;float32&quot;</span>, stop_gradient=<span class="literal">False</span>)</span><br><span class="line">y = paddle.sin(x)</span><br><span class="line">paddle.autograd.backward(y)</span><br><span class="line">dydx = x.grad</span><br><span class="line"></span><br><span class="line">x, y, dydx</span><br><span class="line"></span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.plot(x, dydx)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><code>output[1]</code></p><p><img src="2770491-20230309170957974-778717314.png" alt="img"></p><h2 id="API"><a href="#API" class="headerlink" title="API"></a>API</h2><h3 id="backward"><a href="#backward" class="headerlink" title="backward"></a>backward</h3><p><code>paddle.autograd.backward(tensors, grad_tensors=None, retain_graph=False)</code></p><p>计算给定的 Tensors 的反向梯度。</p><h4 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h4><ul><li>tensors (list[Tensor]) – 将要计算梯度的 Tensors 列表。Tensors 中不能包含有相同的 Tensor。</li><li>grad_tensors (None|list[Tensor|None]，可选) – tensors 的初始梯度值。如果非 None，必须和 tensors 有相同的长度，并且如果其中某一 Tensor 元素为 None，则该初始梯度值为填充 1.0 的默认值；如果是 None，所有的 tensors 的初始梯度值为填充 1.0 的默认值。默认值：None。</li><li>retain_graph (bool，可选) – 如果为 False，反向计算图将被释放。如果在 backward()之后继续添加 OP，需要设置为 True，此时之前的反向计算图会保留。将其设置为 False 会更加节省内存。默认值：False。</li></ul><h4 id="返回"><a href="#返回" class="headerlink" title="返回"></a>返回</h4><p>None</p><h4 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line">x = paddle.to_tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], dtype=<span class="string">&#x27;float32&#x27;</span>, stop_gradient=<span class="literal">False</span>)</span><br><span class="line">y = paddle.to_tensor([[<span class="number">3</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line"></span><br><span class="line">grad_tensor1 = paddle.to_tensor([[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">2</span>, <span class="number">3</span>]], dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">grad_tensor2 = paddle.to_tensor([[<span class="number">1</span>,<span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>]], dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line"></span><br><span class="line">z1 = paddle.matmul(x, y)</span><br><span class="line">z2 = paddle.matmul(x, y)</span><br><span class="line"></span><br><span class="line">paddle.autograd.backward([z1, z2], [grad_tensor1, grad_tensor2], <span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"><span class="comment">#[[12. 18.]</span></span><br><span class="line"><span class="comment"># [17. 25.]]</span></span><br><span class="line"></span><br><span class="line">x.clear_grad()</span><br><span class="line"></span><br><span class="line">paddle.autograd.backward([z1, z2], [grad_tensor1, <span class="literal">None</span>], <span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"><span class="comment">#[[12. 18.]</span></span><br><span class="line"><span class="comment"># [17. 25.]]</span></span><br><span class="line"></span><br><span class="line">x.clear_grad()</span><br><span class="line"></span><br><span class="line">paddle.autograd.backward([z1, z2])</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"><span class="comment">#[[10. 14.]</span></span><br><span class="line"><span class="comment"># [10. 14.]]</span></span><br></pre></td></tr></table></figure><h3 id="grad"><a href="#grad" class="headerlink" title="grad"></a>grad</h3><p><code>paddle.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True, allow_unused=False, no_grad_vars=None)</code></p><p>对于每个 inputs，计算所有 outputs 相对于其的梯度和。</p><h4 id="参数-1"><a href="#参数-1" class="headerlink" title="参数"></a>参数</h4><ul><li>outputs (Tensor|list(Tensor)|tuple(Tensor)) – 用于计算梯度的图的输出变量，或多个输出变量构成的 list/tuple。</li><li>inputs (Tensor|list(Tensor)|tuple(Tensor)) - 用于计算梯度的图的输入变量，或多个输入变量构成的 list/tuple。该 API 的每个返回值对应每个 inputs 的梯度。</li><li>grad_outputs (Tensor|list(Tensor|None)|tuple(Tensor|None)，可选) - outputs 变量梯度的初始值。若 grad_outputs 为 None，则 outputs 梯度的初始值均为全 1 的 Tensor。若 grad_outputs 不为 None，它必须与 outputs 的长度相等，此时，若 grad_outputs 的第 i 个元素为 None，则第 i 个 outputs 的梯度初始值为全 1 的 Tensor；若 grad_outputs 的第 i 个元素为 Tensor，则第 i 个 outputs 的梯度初始值为 grad_outputs 的第 i 个元素。默认值为 None。</li><li>retain_graph (bool，可选) - 是否保留计算梯度的前向图。若值为 True，则前向图会保留，用户可对同一张图求两次反向。若值为 False，则前向图会释放。默认值为 None，表示值与 create_graph 相等。</li><li>create_graph (bool，可选) - 是否创建计算过程中的反向图。若值为 True，则可支持计算高阶导数。若值为 False，则计算过程中的反向图会释放。默认值为 False。</li><li>only_inputs (bool，可选) - 是否只计算 inputs 的梯度。若值为 False，则图中所有叶节点变量的梯度均会计算，并进行累加。若值为 True，则只会计算 inputs 的梯度。默认值为 True。only_inputs=False 功能正在开发中，目前尚不支持。</li><li>allow_unused (bool，可选) - 决定当某些 inputs 变量不在计算图中时抛出错误还是返回 None。若某些 inputs 变量不在计算图中（即它们的梯度为 None），则当 allowed_unused=False 时会抛出错误，当 allow_unused=True 时会返回 None 作为这些变量的梯度。默认值为 False。</li><li>no_grad_vars (Tensor|list(Tensor)|tuple(Tensor)|set(Tensor)，可选) - 指明不需要计算梯度的变量。默认值为 None。</li></ul><h4 id="返回-1"><a href="#返回-1" class="headerlink" title="返回"></a>返回</h4><p>tuple(Tensor)，其长度等于 inputs 中的变量个数，且第 i 个返回的变量是所有 outputs 相对于第 i 个 inputs 的梯度之和。</p><h4 id="代码示例-1"><a href="#代码示例-1" class="headerlink" title="代码示例 1"></a>代码示例 1</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_dygraph_grad</span>(<span class="params">create_graph</span>):</span><br><span class="line">    x = paddle.ones(shape=[<span class="number">1</span>], dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">    x.stop_gradient = <span class="literal">False</span></span><br><span class="line">    y = x * x</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Since y = x * x, dx = 2 * x</span></span><br><span class="line">    dx = paddle.grad(</span><br><span class="line">            outputs=[y],</span><br><span class="line">            inputs=[x],</span><br><span class="line">            create_graph=create_graph,</span><br><span class="line">            retain_graph=<span class="literal">True</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    z = y + dx</span><br><span class="line"></span><br><span class="line">    <span class="comment"># If create_graph = False, the gradient of dx</span></span><br><span class="line">    <span class="comment"># would not be backpropagated. Therefore,</span></span><br><span class="line">    <span class="comment"># z = x * x + dx, and x.gradient() = 2 * x = 2.0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># If create_graph = True, the gradient of dx</span></span><br><span class="line">    <span class="comment"># would be backpropagated. Therefore,</span></span><br><span class="line">    <span class="comment"># z = x * x + dx = x * x + 2 * x, and</span></span><br><span class="line">    <span class="comment"># x.gradient() = 2 * x + 2 = 4.0</span></span><br><span class="line"></span><br><span class="line">    z.backward()</span><br><span class="line">    <span class="keyword">return</span> x.gradient()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(test_dygraph_grad(create_graph=<span class="literal">False</span>)) <span class="comment"># [2.]</span></span><br><span class="line"><span class="built_in">print</span>(test_dygraph_grad(create_graph=<span class="literal">True</span>)) <span class="comment"># [4.]</span></span><br></pre></td></tr></table></figure><h4 id="代码示例-2"><a href="#代码示例-2" class="headerlink" title="代码示例 2"></a>代码示例 2</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_dygraph_grad</span>(<span class="params">grad_outputs=<span class="literal">None</span></span>):</span><br><span class="line">    x = paddle.to_tensor(<span class="number">2.0</span>)</span><br><span class="line">    x.stop_gradient = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    y1 = x * x</span><br><span class="line">    y2 = x * <span class="number">3</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># If grad_outputs=None, dy1 = [1], dy2 = [1].</span></span><br><span class="line">    <span class="comment"># If grad_outputs=[g1, g2], then:</span></span><br><span class="line">    <span class="comment">#    - dy1 = [1] if g1 is None else g1</span></span><br><span class="line">    <span class="comment">#    - dy2 = [1] if g2 is None else g2</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Since y1 = x * x, dx = 2 * x * dy1.</span></span><br><span class="line">    <span class="comment"># Since y2 = x * 3, dx = 3 * dy2.</span></span><br><span class="line">    <span class="comment"># Therefore, the final result would be:</span></span><br><span class="line">    <span class="comment"># dx = 2 * x * dy1 + 3 * dy2 = 4 * dy1 + 3 * dy2.</span></span><br><span class="line"></span><br><span class="line">    dx = paddle.grad(</span><br><span class="line">        outputs=[y1, y2],</span><br><span class="line">        inputs=[x],</span><br><span class="line">        grad_outputs=grad_outputs)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dx.numpy()</span><br><span class="line"></span><br><span class="line">grad_value = paddle.to_tensor(<span class="number">4.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># dy1 = [1], dy2 = [1]</span></span><br><span class="line"><span class="built_in">print</span>(test_dygraph_grad(<span class="literal">None</span>)) <span class="comment"># [7.]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># dy1 = [1], dy2 = [4]</span></span><br><span class="line"><span class="built_in">print</span>(test_dygraph_grad([<span class="literal">None</span>, grad_value])) <span class="comment"># [16.]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># dy1 = [4], dy2 = [1]</span></span><br><span class="line"><span class="built_in">print</span>(test_dygraph_grad([grad_value, <span class="literal">None</span>])) <span class="comment"># [19.]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># dy1 = [3], dy2 = [4]</span></span><br><span class="line">grad_y1 = paddle.to_tensor(<span class="number">3.0</span>)</span><br><span class="line"><span class="built_in">print</span>(test_dygraph_grad([grad_y1, grad_value])) <span class="comment"># [24.]</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> paddlepaddle </tag>
            
            <tag> 飞桨 </tag>
            
            <tag> 自动求导 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
